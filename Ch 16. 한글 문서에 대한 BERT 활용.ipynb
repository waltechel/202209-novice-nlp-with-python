{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9ca054",
   "metadata": {},
   "source": [
    "# Ch 16. 한글문서에 대한 BERT 활용\n",
    "\n",
    "\n",
    "## 16.1 다중 언어를 위한(multilingual) BERT 사전학습모형의 미세조정학습을 이용한 한글 문서의 분류\n",
    "\n",
    "\n",
    "(1) https://github.com/google-research/bert/blob/master/multilingual.md\n",
    "<br>(2) https://huggingface.co/transformers/pretrained_models.html\n",
    "<br>(3) https://github.com/SKTBrain/KoBERT\n",
    "<br>(4) https://github.com/Beomi/KcBERT\n",
    "<br>(5) https://github.com/Beomi/KcELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaf66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# rating이 6보다 작으면 0 즉 부정, 6 이상이면 긍정으로 라벨 생성\n",
    "y = [0 if rate < 6 else 1 for rate in df.rating]\n",
    "# 데이터셋을 학습, 검증, 평가의 세 데이터셋으로 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160ac054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5042229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e8780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8282\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2072\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2072' max='2072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2072/2072 11:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.541876</td>\n",
       "      <td>0.789207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.485679</td>\n",
       "      <td>0.813473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.480119</td>\n",
       "      <td>0.796813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>0.439106</td>\n",
       "      <td>0.819268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2761\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2761\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2761\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2761\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2072, training_loss=0.510447410082725, metrics={'train_runtime': 700.1548, 'train_samples_per_second': 23.658, 'train_steps_per_second': 2.959, 'total_flos': 2689808985606720.0, 'train_loss': 0.510447410082725, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# bert-base-multilingual-cased 사전학습모형으로부터 분류기 모형을 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Trainer에서 사용할 하이퍼 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n",
    "    num_train_epochs=2,              # 학습 에포크 수\n",
    "    evaluation_strategy=\"steps\",      # epoch마다 검증 데이터셋에 대한 평가 지표를 출력\n",
    "    eval_steps = 500,                # \n",
    "    per_device_train_batch_size=8,   # 학습에 사용할 배치 사이즈\n",
    "    per_device_eval_batch_size=16,   # 평가에 사용할 배치 사이즈\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 학습할 모형\n",
    "    args=training_args,              # 위에서 정의한 학습 매개변수\n",
    "    train_dataset=train_dataset,     # 훈련 데이터셋\n",
    "    eval_dataset=val_dataset,        # 검증 데이터셋\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 미세조정학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecaf4d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/774948626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b83935",
   "metadata": {},
   "source": [
    "아래와 같이 미세조정학습을 마친 모형으로 평가 데이터셋에 대해 성능을 측정해본다. 80.1% 정도로 나쁘지 않은 성능을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5512b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3682\n",
      "  Batch size = 16\n",
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/231 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.45343217253685,\n",
       " 'eval_accuracy': 0.8009234111895709,\n",
       " 'eval_runtime': 37.1026,\n",
       " 'eval_samples_per_second': 99.238,\n",
       " 'eval_steps_per_second': 6.226,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528db2",
   "metadata": {},
   "source": [
    "## 16.2 KoBERT 사전학습모형에 대한 파이토치 기반 미세조정학습\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4c0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c9af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/spiece.model from cache at C:\\Users\\ParkSangun/.cache\\huggingface\\transformers\\6920ce54223b52af14e36b32047ced34c47ec88ac51f45ce0141aaa1054e3263.7eed87d19282a93a2d45e130f20b4d8e831cbf8e957f1476628fd4ab99ae977f\n",
      "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/special_tokens_map.json from cache at C:\\Users\\ParkSangun/.cache\\huggingface\\transformers\\2ad28172340bc816ccd4ffc7a51682e0c5c89a88a0d618ab40eeb81a3980b356.3db0799720217f7da35e92d033f167ac40c8d2c02fa035130b7bb070f6355074\n",
      "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/tokenizer_config.json from cache at C:\\Users\\ParkSangun/.cache\\huggingface\\transformers\\ebd8df8703bef77849f188d10c4ca994fd45a1b41518401a08c13487ef55c723.55c5c51d9ae1a9f730238c32bd0fa05b12cd7d99d757ee0e6accc4c6e4085f40\n",
      "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/tokenizer.json from cache at None\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '.', '▁반', '갑', '습니다', '.']\n",
      "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5345, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82152a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/skt/kobert-base-v1/resolve/main/config.json from cache at C:\\Users\\ParkSangun/.cache\\huggingface\\transformers\\b8898dabd49ed32401ee6a6bc5eb011f12728750b44d08b151acf270bf1732ca.1007ab583c49854e3c65c61288a980ae4d25a4bbfa51b51915ec1772f02f992d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"kobert_version\": 1.0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/skt/kobert-base-v1/resolve/main/pytorch_model.bin from cache at C:\\Users\\ParkSangun/.cache\\huggingface\\transformers\\29f85cb7266d82cd889c6f5db4d439fb71b42c237c56f56fce78e950c7d4a2e5.b39ec656f15b262c43c61e29d10204da76efaf1f5535892e2c90431065f17dba\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at skt/kobert-base-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# KoBERT 사전학습모형 로드\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# BERT를 포함한 신경망 모형\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels): \n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        # 분류기 정의\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # BERT 모형에 입력을 넣고 출력을 받음\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "# token_size는 BERT 토큰과 동일, bert_model.config.hidden_size로 알 수 있음\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94646422",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee83e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, elapsed time: 141.06, train loss: 0.4933, validation loss: 0.4198\n",
      "Step 1000, elapsed time: 281.86, train loss: 0.3552, validation loss: 0.3710\n",
      "Step 1500, elapsed time: 423.89, train loss: 0.2663, validation loss: 0.2829\n",
      "Step 2000, elapsed time: 566.67, train loss: 0.2225, validation loss: 0.2967\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)  # 모형을 GPU로 복사\n",
    "model.train()     # 학습모드로 전환\n",
    "\n",
    "# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()    # 멀티클래스이므로 크로스 엔트로피를 손실함수로 사용\n",
    "\n",
    "num_epochs = 2      # 학습 epoch를 3회로 설정\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "# 학습 스케줄러 설정\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "start = time.time() # 시작시간 기록\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #total_epoch_loss = 0  # epoch의 총 loss 초기화\n",
    "    for batch in train_loader:\n",
    "        model.train()     # 학습모드로 전환\n",
    "        optim.zero_grad()     # 그래디언트 초기화\n",
    "        # 배치에서 label을 제외한 입력만 추출하여  GPU로 복사\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n",
    "        labels = batch['labels'].to(device) # 배치에서 라벨을 추출하여 GPU로 복사\n",
    "        outputs = model(inputs) # 모형으로 결과 예측\n",
    "        # 두 클래스에 대해 예측하고 각각 비교해야 하므로 labels에 대해 원핫인코딩을 적용한 후에 손실을 게산\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "\n",
    "        train_loss += loss\n",
    "        loss.backward() # 그래디언트 계산\n",
    "        optim.step()    # 가중치 업데이트\n",
    "        scheduler.step() # 스케줄러 업데이트\n",
    "        \n",
    "        step += 1\n",
    "        if step % eval_steps == 0: # eval_steps 마다 경과한 시간과 loss를 출력\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_train_loss = train_loss / eval_steps\n",
    "            elapsed = time.time() - start\n",
    "            print('Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' \n",
    "                  % (step, elapsed, avg_train_loss, avg_val_loss))\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3df14ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKSA~1\\AppData\\Local\\Temp/ipykernel_4200/2721578045.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8693644758283542}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric= load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad(): # 학습할 필요가 없으므로 그래디언트 계산을 끔\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f60783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
