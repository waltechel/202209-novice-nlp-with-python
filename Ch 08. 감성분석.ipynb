{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. 감성 분석\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 감성 사전을 이용한 영화 리뷰 감성 분석\n",
    "\n",
    "### 8.2.1 NLTK 영화 리뷰 데이터 준비\n",
    "\n",
    "1: https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count: 2000\n",
      "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "#categories of reviews: ['neg', 'pos']\n",
      "#num of \"neg\" reviews: 1000\n",
      "#num of \"pos\" reviews: 1000\n",
      "#id of the first review: neg/cv000_29416.txt\n",
      "#part of the first review: plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
      "#sentiment of the first review: ['neg']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('#id of the first review:', fileid)\n",
    "print('#part of the first review:', movie_reviews.raw(fileid)[:500]) #첫번째 문서의 내용을 500자까지만 출력\n",
    "print('#sentiment of the first review:', movie_reviews.categories(fileid)) #첫번째 문서의 감성\n",
    "\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 TextBlob을 이용한 감성 분석\n",
    "\n",
    "1: https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from textblob) (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: click in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.06479782948532947, subjectivity=0.5188408350908352)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "result = TextBlob(reviews[0])\n",
    "print(result.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_TextBlob(docs):\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        testimonial = TextBlob(doc)\n",
    "        if testimonial.sentiment.polarity > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#TextBlob을 이용한 리뷰 감성분석의 정확도: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('#TextBlob을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_TextBlob(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 AFINN을 이용한 감성 분석\n",
    "\n",
    "https://github.com/fnielsen/afinn \n",
    "\n",
    "(1) http://corpustext.com/reference/sentiment_afinn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: afinn in c:\\users\\sk8er\\anaconda3\\envs\\textmining\\lib\\site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Afinn을 이용한 리뷰 감성분석의 정확도: 0.664\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "def sentiment_Afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        if afn.score(doc) > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results\n",
    "\n",
    "print('#Afinn을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_Afinn(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 VADER를 이용한 감성 분석\n",
    "\n",
    "(1) https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Vader을 이용한 리뷰 감성분석의 정확도: 0.635\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_vader(docs):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        score = analyser.polarity_scores(doc)\n",
    "        if score['compound'] > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "\n",
    "    return results\n",
    "\n",
    "print('#Vader을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_vader(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5 한글 감성사전\n",
    "\n",
    "1: https://github.com/park1200656/KnuSentiLex   \n",
    "2: https://github.com/mrlee23/KoreanSentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 학습을 통한 머신러닝 기반의 감성 분석\n",
    "\n",
    "### 8.3.1 NLTK 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=7)\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set dimension: (1600, 36189)\n",
      "#Test set dimension: (400, 36189)\n",
      "#Train set score: 0.998\n",
      "#Test set score: 0.797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(X_train) \n",
    "\n",
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('#Train set dimension:', X_train_tfidf.shape) # 실제로 몇개의 특성이 사용되었는지 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('#Test set dimension:', X_test_tfidf.shape)\n",
    "\n",
    "NB_clf = MultinomialNB(alpha=0.01) # 분류기 선언\n",
    "NB_clf.fit(X_train_tfidf, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "print('#Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('#Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test))) #test set에 대한 예측정확도를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 다음 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df.rating.value_counts().sort_index().plot(kind='bar')\n",
    "#df.rating.plot.hist(bins=10) #히스토그램을 그릴 수도 있다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| | 긍정으로 예측한 리뷰(PP) | 부정으로 예측한 리뷰(PN) |\n",
    "|---|---|---|\n",
    "|실제 긍정인 리뷰(P) | True positive(TP) | False negative(FN) |\n",
    "|실제 부정인 리뷰(N) | False positive(FP) | True negative(TN) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 11043\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.rating, random_state=7)\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Test set size:', len(X_test))\n",
    "\n",
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "okt = Okt()\n",
    "\n",
    "def twit_tokenizer(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in okt.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, max_features=2000, min_df=5, max_df=0.5) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Regression Train set R2 score: 0.605\n",
      "#Regression Test set R2 score: 0.395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()  #객체를 생성\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "print('#Regression Train set R2 score: {:.3f}'.format(lr.score(X_train_tfidf, y_train)))\n",
    "print('#Regression Test set R2 score: {:.3f}'.format(lr.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.888\n",
      "#Precision for train set: 0.893\n",
      "#Recall for train set: 0.969\n",
      "#F1 for train set: 0.929\n",
      "#Accuracy for test set: 0.848\n",
      "#Precision for test set: 0.868\n",
      "#Recall for test set: 0.946\n",
      "#F1 for test set: 0.905\n"
     ]
    }
   ],
   "source": [
    "y_train_senti = (y_train > 5)\n",
    "y_test_senti = (y_test > 5)\n",
    "\n",
    "y_train_predict = (lr.predict(X_train_tfidf) > 5)\n",
    "y_test_predict = (lr.predict(X_test_tfidf) > 5)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.878\n",
      "#Precision for train set: 0.878\n",
      "#Recall for train set: 0.973\n",
      "#F1 for train set: 0.923\n",
      "#Accuracy for test set: 0.855\n",
      "#Precision for test set: 0.866\n",
      "#Recall for test set: 0.958\n",
      "#F1 for test set: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train_senti) # train data를 이용하여 분류기를 학습\n",
    "\n",
    "y_train_predict = LR_clf.predict(X_train_tfidf)\n",
    "y_test_predict = LR_clf.predict(X_test_tfidf)\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 참고문헌\n",
    "\n",
    "Finn Årup Nielsen A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May.\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
