{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltechel/202209-novice-nlp-with-python/blob/master/Ch%2002.%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzTlTge0gAsJ"
      },
      "source": [
        "# Chapter 2. 텍스트 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 텍스트 전처리의 개념\n",
        "\n",
        "### 2.1.1 왜 전처리가 필요한가?\n",
        "\n",
        "텍스트에는 불필요한 정보도 많이 있으며, 작업 목적에 맞지 않는 정보를 사전에 정리하는 것이 효율적이기 때문이다.\n",
        "\n",
        "자연어를 벡터화하는 것을 임베딩이라고 하는데, 임베딩을 더 잘 하기 위해서도 텍스트 전처리가 필요하다."
      ],
      "metadata": {
        "id": "cE2--fIY0u9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 전처리의 단계\n",
        "\n",
        "1. 정제 : 노이즈나 불용어를 제거한다.\n",
        "2. 토큰화 : 텍스트를 원하는 단위로 분할한다.\n",
        "3. 정규화 : 어간을 추출하거나 표제어를 추출한다.\n",
        "4. 품사 태깅 : 정규화된 목록에 품사를 태깅하여 메타 정보를 등록한다.\n"
      ],
      "metadata": {
        "id": "C7fMhHYv00jy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o8hdLO2gAsT"
      },
      "source": [
        "## 2. 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtUHFi65gAsV"
      },
      "source": [
        "#### NLTK (https://www.nltk.org/) 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rpwoFgEMgAsX",
        "outputId": "a7321cdc-c0ec-4c5c-c64a-b45351a104a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 필요한 nltk library download\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('webtext')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR8nbQ2CgAsc"
      },
      "source": [
        "### 2.1 문장 토큰화(sentence tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x3r4IT2-gAsd"
      },
      "outputs": [],
      "source": [
        "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b6kJJfBigAsf",
        "outputId": "3ed98724-46b7-481b-b588-6e252dc1741c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 등을 이용\n",
        "print(sent_tokenize(para)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tI5xccM-gAsh",
        "outputId": "8bb70226-56f6-49b8-e707-436ceca09e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
          ]
        }
      ],
      "source": [
        "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
        "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
        "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
        "\n",
        "import nltk.data\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
        "print(tokenizer.tokenize(paragraph_french))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sb35KTR-gAsj"
      },
      "outputs": [],
      "source": [
        "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RnwE3NyJgAsl",
        "outputId": "0e35cb83-f4f4-4cf7-ec72-8e9fd35d5adf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
          ]
        }
      ],
      "source": [
        "print(sent_tokenize(para_kor)) #한국어에 대해서도 sentence tokenizer는 잘 동작함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c07WP3uEgAsm"
      },
      "source": [
        "### 2.2 단어 토큰화 (word tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어에 대해서는 마침표나 느낌표가 토큰으로 갈라지는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "KN2xSFXmg8N8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F6R8X4VegAso",
        "outputId": "370b6066-98ed-42e3-ae4f-9241fa46eeb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#주어진 text를 word 단위로 tokenize함\n",
        "print(word_tokenize(para)) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- word_tokenize와 달리 WordPunctTokenizer 는 어퍼스트로피도 분리하는 것을 확인할 수 있다. "
      ],
      "metadata": {
        "id": "Ma4w81Logpg6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_fhAeVQPgAsp",
        "outputId": "8a1c698e-e47e-44d3-b6e8-6d42756e69b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "print(WordPunctTokenizer().tokenize(para))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국어는 토큰화를 어절 단위로밖에 수행하지 못하는 것을 알 수 있다.\n",
        "  - 한국어의 경우 형태소 단위까지 분절이 가능한데 한국어에서는 KoNLPy를 활용해서 분석해본다."
      ],
      "metadata": {
        "id": "R3yozopwhOXL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8NxvVdhdgAsq",
        "outputId": "efd8e9bc-308b-4bb8-d5ef-a9e6021d2424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
          ]
        }
      ],
      "source": [
        "print(word_tokenize(para_kor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RviMz_eEgAsr"
      },
      "source": [
        "### 2.3 정규표현식을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `are` 에서 a를 가져오고, `boy`에서 b를 가져왔다."
      ],
      "metadata": {
        "id": "dF0qwamRhipY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vKpZZIySgAss",
        "outputId": "58571b99-8df4-44e6-b5bc-c11c2a33e4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re\n",
        "re.findall(\"[abc]\", \"How are you, boy?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 숫자에 해당하는 것을 모두 가져왔다."
      ],
      "metadata": {
        "id": "fSk6JRxhhtIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "foJnSl2ugAst",
        "outputId": "bc5568aa-a260-485a-b61a-837d89f90cdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', '7', '5', '9']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "re.findall(\"[0-9]\", \"3a7b5c9d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 알파벳과 숫자에 _언더바 까지 모두 검색하고 싶다면 `[a-zA-Z0-9_]` 로 표기하면 된다.\n",
        "  - 약어로는 `[\\w]` 를 활용한다."
      ],
      "metadata": {
        "id": "aXbq_fp5h17O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DG0PXL89gAsu",
        "outputId": "f35450e2-5f27-48c5-837d-835e4adfee95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "re.findall(\"[\\w]\", \"3a 7b_ '.^&5c9d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한 번 이상의 반복을 나타내고자 할 때는 `+`를 사용할 수 있다."
      ],
      "metadata": {
        "id": "pWyzpERfiHWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mOuLH0iggAsv",
        "outputId": "314fa38c-9173-4945-f229-b96ac57b5a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_', '__', '___']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "re.findall(\"[_]+\", \"a_b, c__d, e___f\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 센스 있게 문장 부호와 스페이스를 제거하는 방법"
      ],
      "metadata": {
        "id": "ucqPFJNciYHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0pQwZNN5gAsv",
        "outputId": "8b5a3eeb-6a1d-4e6d-9aa7-d946d5b71866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How', 'are', 'you', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "re.findall(\"[\\w]+\", \"How are you, boy?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `o`의 개수가 2개부터 4개까지 찾아보는 방법\n",
        "  - 맨 마지막의 경우 4개, 3개로 분리된 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "yv8WMZtPidZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ro8g1UstgAsw",
        "outputId": "a6f08573-9f67-4d98-a414-810276dcba4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oo', 'oooo', 'oooo', 'ooo']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "re.findall(\"[o]{2,4}\", \"oh, hoow are yoooou, boooooooy?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_QjUncENgAsx",
        "outputId": "892c971b-22b8-4455-8c48-9ca2c62bdf80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'I', \"can't\", 'go', 'there']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\") #regular expression(정규식)을 이용한 tokenizer\n",
        "#단어단위로 tokenize \\w:문자나 숫자를 의미 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄\n",
        "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))\n",
        "# can't를 하나의 단어로 인식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Og-dVerHgAsy",
        "outputId": "de791b19-4994-41c3-fc6a-f2e73b85d35c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'I', 'can', 't', 'go', 'there']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\w]+\") \n",
        "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모두 소문자로 바꾸고 ' 를 포함해 세 글자 이상의 단어들만 골라내고 싶을 때는 다음과 같이 한다."
      ],
      "metadata": {
        "id": "sUm62qlPuP8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yv21kPXFgAsz",
        "outputId": "de07b5b2-5493-4d93-93a6-0d658d7854b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"can't\", 'there']\n"
          ]
        }
      ],
      "source": [
        "text1 = \"Sorry, I can't go there.\"\n",
        "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") \n",
        "print(tokenizer.tokenize(text1.lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_hpulrgAs0"
      },
      "source": [
        "### 2.4 노이즈와 불용어 제거\n",
        "\n",
        "- 노이즈는 특수문자 혹은 오타를 말한다.\n",
        "- 불용어는 실제 사용은 되는 단어들이지만 분석에 별다른 영향을 미치지 않는 단어들을 말한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0dhXM7GRgAs0",
        "outputId": "6f0356ad-9db7-417b-98a9-95c4dbe2d2b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'during', 'yourself', 'hers', 'will', 'i', 'here', 'then', 'wasn', 'am', 'their', 'can', 'so', \"you've\", \"don't\", \"wouldn't\", 'more', 'is', 'should', 'be', 'further', 'hadn', \"needn't\", 'until', \"doesn't\", 'once', 'these', 'was', 'couldn', 'our', \"hasn't\", 'off', 'this', 's', 'but', 'of', 'you', \"she's\", 'not', 'all', 'ain', 'and', 'had', 've', 'only', 'mightn', 'your', 'ours', 'she', 're', 'themselves', 'needn', \"you'd\", 'while', 'are', 'why', 'those', 'into', 'doesn', 'who', 'y', 'now', 'yourselves', 'for', 'being', 'very', 'from', 'an', 'a', 'the', 'own', \"won't\", 'same', \"should've\", 'down', 'o', 'again', 'just', 'each', 'doing', 'both', 'hasn', \"aren't\", \"shan't\", 'or', 'about', \"you'll\", 'by', 'below', 'haven', 'her', 'which', 'shouldn', 'on', 'me', 'mustn', 'shan', 'that', 'between', 'any', 'him', 'than', 'too', 'll', 'herself', 'when', 'there', 'as', 'm', 'over', 'no', 'it', 'its', \"hadn't\", 'before', 'd', 'weren', 'don', 'my', 'his', 'itself', 'theirs', \"it's\", \"that'll\", 'does', 'did', 'won', 'above', 'having', 'ourselves', 'how', 'against', 'in', 'wouldn', \"mustn't\", \"mightn't\", 'were', \"haven't\", 'ma', \"couldn't\", 'yours', 'nor', 'what', 'been', 'other', 'them', 'whom', 'after', 'under', 'himself', 'have', 'where', 'they', 't', 'few', \"didn't\", \"weren't\", 'at', 'do', \"you're\", 'out', 'with', 'myself', 'has', 'because', 'to', 'aren', \"shouldn't\", 'most', \"isn't\", 'such', 'some', 'didn', 'he', 'isn', 'through', \"wasn't\", 'up', 'if', 'we'}\n",
            "['sorry', 'i', \"couldn't\", 'go', 'to', 'movie', 'yesterday']\n",
            "['sorry', 'go', 'movie', 'yesterday']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
        "english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환\n",
        "\n",
        "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokens = tokenizer.tokenize(text1.lower()) #word_tokenize로 토큰화\n",
        "\n",
        "print(english_stops)\n",
        "print(tokens)\n",
        "result = [word for word in tokens if word not in english_stops] #stopwords를 제외한 단어들만으로 list를 생성\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어 불용어는 다음과 같다."
      ],
      "metadata": {
        "id": "LMwCQntKvnZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5JzmoSTrgAs1",
        "outputId": "68820a49-e3d4-482b-c6f3-0bcea359d15e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'during', 'yourself', 'hers', 'will', 'i', 'here', 'then', 'wasn', 'am', 'their', 'can', 'so', \"you've\", \"don't\", \"wouldn't\", 'more', 'is', 'should', 'be', 'further', 'hadn', \"needn't\", 'until', \"doesn't\", 'once', 'these', 'was', 'couldn', 'our', \"hasn't\", 'off', 'this', 's', 'but', 'of', 'you', \"she's\", 'not', 'all', 'ain', 'and', 'had', 've', 'only', 'mightn', 'your', 'ours', 'she', 're', 'themselves', 'needn', \"you'd\", 'while', 'are', 'why', 'those', 'into', 'doesn', 'who', 'y', 'now', 'yourselves', 'for', 'being', 'very', 'from', 'an', 'a', 'the', 'own', \"won't\", 'same', \"should've\", 'down', 'o', 'again', 'just', 'each', 'doing', 'both', 'hasn', \"aren't\", \"shan't\", 'or', 'about', \"you'll\", 'by', 'below', 'haven', 'her', 'which', 'shouldn', 'on', 'me', 'mustn', 'shan', 'that', 'between', 'any', 'him', 'than', 'too', 'll', 'herself', 'when', 'there', 'as', 'm', 'over', 'no', 'it', 'its', \"hadn't\", 'before', 'd', 'weren', 'don', 'my', 'his', 'itself', 'theirs', \"it's\", \"that'll\", 'does', 'did', 'won', 'above', 'having', 'ourselves', 'how', 'against', 'in', 'wouldn', \"mustn't\", \"mightn't\", 'were', \"haven't\", 'ma', \"couldn't\", 'yours', 'nor', 'what', 'been', 'other', 'them', 'whom', 'after', 'under', 'himself', 'have', 'where', 'they', 't', 'few', \"didn't\", \"weren't\", 'at', 'do', \"you're\", 'out', 'with', 'myself', 'has', 'because', 'to', 'aren', \"shouldn't\", 'most', \"isn't\", 'such', 'some', 'didn', 'he', 'isn', 'through', \"wasn't\", 'up', 'if', 'we'}\n"
          ]
        }
      ],
      "source": [
        "print(english_stops) #nltk가 제공하는 영어 stopword를 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 자신만의 불용어를 만들고 싶으면 다음과 같이 진행한다."
      ],
      "metadata": {
        "id": "8pY56ZoyvsI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jV_Rwz9SgAs1",
        "outputId": "d7c4f98a-82f8-49bd-95d1-eeefa32342ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
          ]
        }
      ],
      "source": [
        "#자신만의 stopwords를 만들고 이용\n",
        "#한글처리에서도 유용하게 사용할 수 있음\n",
        "my_stopword = ['i', 'go', 'to'] #나만의 stopword를 리스트로 정의\n",
        "result = [word for word in tokens if word not in my_stopword] \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivpHuJaBgAs2"
      },
      "source": [
        "# 2. 정규화(Normalization)\n",
        "\n",
        "- 정규화    \n",
        "  정규화는 같은 의미를 가진 동일한 단어이면서 다른 형태로 쓰여진 단어들을 통일해 표준 단어로 만드는 작업을 말한다.    \n",
        "  어휘의 크기를 줄이는 기법인 정규화는 비슷한 토큰들을 하나의 형태로 결합시키는 것으로, 결국 어휘의 종류가 줄어들게 되어 과대적합이 일어날 가능성 또한 줄여준다.\n",
        "  - 정규화는 P로 분류하는 대상을 확대함으로써 재현율을 높여주지만 정밀도를 낮출 수 있다.\n",
        "    - 재현율(recall) = TP / (TP + FN)\n",
        "    - 정밀도(precision) = TP / (TP + FP)\n",
        "\n",
        "## 2.1 어간 추출(Stemming)\n",
        "\n",
        "- 어간 추출    \n",
        "  어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해내는 작업을 말한다. \n",
        "  - 어형 : 단어의 형태\n",
        "  - 어간 : 어형변화에서 변화하지 않는 부분\n",
        "  - 어미 : 어형변화에서 바뀌는 부분\n",
        "  - 어형변화\n",
        "    - 통시적 어형변화 : 간다 -> 갔다 같은 시제가 반영된 어형변화\n",
        "    - 공시적 어형변화 : 작다 -> 작고 같은 시제가 반영되지 않은 어형변화\n",
        "\n",
        "- 영어의 어간추출 알고리즘으로는 포터 스테머(Porter Stemmer), 랭카스터 스테머(Lancaster Stemmer)가 존재한다. 아래는 포터 스테머를 활용한 어간추출 소스 코드이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jO4YtAlxgAs3",
        "outputId": "5bf14037-e00e-4007-a3a2-2805926fafaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cook cookeri cookbook\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다음은 포터 스테머를 활용하여 어간 추출한 결과를 나타낸다."
      ],
      "metadata": {
        "id": "WZ9kWKa-xErW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRykSs0NgAs3",
        "outputId": "95a3267c-08aa-4c5c-f06f-ea7a1d2f97c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
            "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'min', 'class', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
        "tokens = word_tokenize(para) #토큰화 실행\n",
        "print(tokens)\n",
        "result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다음은 랭카스터 스테머를 사용해 어간 추출한 결과는 다음과 같다."
      ],
      "metadata": {
        "id": "mksnmufXxKo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "h5Qg5JOcgAs4",
        "outputId": "670b3968-a965-41fd-93ce-2d792f22c930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cook cookery cookbook\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
        "tokens = word_tokenize(para) #토큰화 실행\n",
        "print(tokens)\n",
        "result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tbfJcNWuxJ7V",
        "outputId": "28d1b89c-1028-464e-c047-fd4e575b00d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
            "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'min', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDWWV91gAs5"
      },
      "source": [
        "## 2.2 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 표제어    \n",
        "  사전에 나오는 말을 표제어라고 한다. 표제어 추출은 의미적 관점에서 단어의 기본형을 찾는 작업을 말한다.\n",
        "  - 어간이 표제어일 필요는 없다. "
      ],
      "metadata": {
        "id": "TbWiCdaFxfH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kDKCfo0qgAs5",
        "outputId": "cb44299e-1c5b-4544-9d4e-2d1a536fe815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cooking\n",
            "cook\n",
            "cooking\n",
            "cookery\n",
            "cookbook\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('cooking'))\n",
        "print(lemmatizer.lemmatize('cooking', pos='v')) #품사를 지정\n",
        "print(lemmatizer.lemmatize('cooking', pos='n')) #품사를 지정\n",
        "print(lemmatizer.lemmatize('cookery'))\n",
        "print(lemmatizer.lemmatize('cookbooks'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어간과 표제어 간의 차이는 다음과 같다."
      ],
      "metadata": {
        "id": "3jStfRcfyAHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "id": "ZXnpPum-gAs7",
        "outputId": "5fc496b2-04b2-49ab-b14c-a0ad6c33bc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemming result: believ\n",
            "lemmatizing result: belief\n",
            "lemmatizing result: believe\n"
          ]
        }
      ],
      "source": [
        "#comparison of lemmatizing and stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print('stemming result:', stemmer.stem('believes'))\n",
        "print('lemmatizing result:', lemmatizer.lemmatize('believes'))\n",
        "print('lemmatizing result:', lemmatizer.lemmatize('believes', pos='v'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poBA-HAggAs7"
      },
      "source": [
        "# 3. 품사 태깅(Part-of-Speech Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5DsgsxCgAs8"
      },
      "source": [
        "## 3.1 품사의 이해\n",
        "\n",
        "- 품사는 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 부사, 감탄사와 같이 공통된 성질을 지닌 낱말끼리 모아 놓은 낱말의 갈래를 말한다.\n",
        "- 낱말은 뜻을 가지고 홀로 쓰일 수 있는 말의 가장 작은 단위"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIKXfeoWgAtQ"
      },
      "source": [
        "## 3.2 NLTK를 이용한 품사 태깅"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nltk.pos_tag()`는 토큰화된 결과에 대해 품사를 태깅해 (단어, 품사) 로 구성된 튜플의 리스트로 품사 태깅 결과를 반환해준다."
      ],
      "metadata": {
        "id": "TsOd2WGAyzx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6hAkOTr_gAtR",
        "outputId": "c0a71388-cfb0-4384-e4e9-533626680dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
        "print(nltk.pos_tag(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 품사의 약어를 잘 모를 경우에는 아래와 같이 품사 약어의 의미와 그 설명을 알 수 있다."
      ],
      "metadata": {
        "id": "biKpfu_ozEem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vHnVlh9AgAtR",
        "outputId": "8cd8be5a-b50b-4fcb-af35-4551ea4289d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('NNP')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래와 같이 원하는 품사의 단어들만 추출할 수도 있다."
      ],
      "metadata": {
        "id": "7DrcN4V7zK1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "P982pXJbgAtS",
        "outputId": "9396771f-d6b6-4a8b-89ab-3368118f8373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
          ]
        }
      ],
      "source": [
        "my_tag_set = ['NN', 'VB', 'JJ']\n",
        "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
        "print(my_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래와 같이 단어에 품사 정보를 추가해 구분해볼 수도 있다."
      ],
      "metadata": {
        "id": "bQpWcyYNzP87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qZ7GRkZ3gAtT",
        "outputId": "01fbcb60-709b-4307-b723-6a49edc77fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
          ]
        }
      ],
      "source": [
        "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
        "print(words_with_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tqynKPWgAtT"
      },
      "source": [
        "## 3.3 한글 형태소 분석과 품사 태깅"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한글 형태소 분석과 품사 태깅\n",
        "\n",
        "- NLTK로는 한글 토큰화나 품사 태깅이 잘 되지 않는다. \n",
        "- 가령 `절망의`는 `절망` + `의` 가 결합되어 있어 한글 토큰화에 실패하였다.\n",
        "- `절망의` 에 `JJ` 가 표기되었는데 이는 형용사로써, 물론 옳지 못하다(명사 + 조사 가 맞다)"
      ],
      "metadata": {
        "id": "FKHcieY_zlE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Kq8fpVMxgAtU"
      },
      "outputs": [],
      "source": [
        "sentence = '''절망의 반대가 희망은 아니다.\n",
        "어두운 밤하늘에 별이 빛나듯\n",
        "희망은 절망 속에 싹트는 거지\n",
        "만약에 우리가 희망함이 적다면\n",
        "그 누가 세상을 비출어줄까.\n",
        "정희성, 희망 공부'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "espG1FBAgAtV",
        "outputId": "6f067fbb-4b89-4e4d-cbd3-578e81598bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
            "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n",
        "print(nltk.pos_tag(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('JJ')"
      ],
      "metadata": {
        "id": "Srke91j00Dg1",
        "outputId": "3ab4a926-e8e4-4e28-d96e-9e2ecc2841e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKfDeun1gAtW"
      },
      "source": [
        "### KoNLPy 설치\n",
        "\n",
        "https://konlpy.org/ko/latest/install/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 코랩에는 konlpy 가 설치되어 있지 않으므로 설치를 해줘야 햔다."
      ],
      "metadata": {
        "id": "uiWBBoB80cRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "mVYNKKC90Sgr",
        "outputId": "5dd02ff0-10d9-4ca2-869b-070654128f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 301 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sB0ymF8dgAtW"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "t = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "scrolled": true,
        "id": "Gx5eeQ8OgAtX",
        "outputId": "8cba734f-d264-4446-a5bd-82bc77eed95a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
            "\n",
            "명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
            "\n",
            "품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
          ]
        }
      ],
      "source": [
        "print('형태소:', t.morphs(sentence))\n",
        "print()\n",
        "print('명사:', t.nouns(sentence))\n",
        "print()\n",
        "print('품사 태깅 결과:', t.pos(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOtA1GAlgAtY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}